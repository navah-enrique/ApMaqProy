{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Survival Model to MLflow\n",
    "\n",
    "@roman_avj\n",
    "\n",
    "7 nov 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import sqlalchemy\n",
    "import mlflow\n",
    "import cloudpickle\n",
    "\n",
    "\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from xgbse import XGBSEStackedWeibull\n",
    "from xgbse.extrapolation import extrapolate_constant_risk\n",
    "import lifelines\n",
    "\n",
    "from scipy.integrate import simpson\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "from xgbse.metrics import (\n",
    "    approx_brier_score,\n",
    "    dist_calibration_score,\n",
    "    concordance_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47397 entries, 0 to 47396\n",
      "Columns: 141 entries, id to cosine_tmonth\n",
      "dtypes: datetime64[us](2), float64(122), int32(1), int64(4), object(9), string(3)\n",
      "memory usage: 50.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "df_model = pd.read_parquet('../../data/data2analyze_clean_v2_sale.parquet')\n",
    "df_model.info()\n",
    "\n",
    "# add if has maintenance\n",
    "df_model['has_maintenance'] = df_model['cost_of_maintenance'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# clip columns with 'lag' up to 99 percentile\n",
    "vars_lag = df_model.columns[df_model.columns.str.contains('lag')]\n",
    "df_model[vars_lag] = df_model[vars_lag].clip(upper=df_model[vars_lag].quantile(0.99), axis=1)\n",
    "\n",
    "# look rows with maximum time2event\n",
    "df_max = df_model[df_model['time2event'] == df_model['time2event'].max()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47397 entries, 0 to 47396\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   property_type          47397 non-null  category\n",
      " 1   first_price            47397 non-null  float64 \n",
      " 2   diff_first_prediction  47397 non-null  float64 \n",
      " 3   page_on_marketplace    47397 non-null  float64 \n",
      " 4   latitude               47397 non-null  float64 \n",
      " 5   longitude              47397 non-null  float64 \n",
      " 6   woe_marketplace        47397 non-null  float64 \n",
      " 7   woe_seller             47397 non-null  float64 \n",
      " 8   woe_id_sepomex         47397 non-null  float64 \n",
      " 9   sine_tmonth            47397 non-null  float64 \n",
      " 10  cosine_tmonth          47397 non-null  float64 \n",
      "dtypes: category(1), float64(10)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "# select columns\n",
    "vars_x_categorical = ['property_type']\n",
    "vars_x_discrete = []\n",
    "vars_x_woe = ['woe_marketplace', 'woe_seller', 'woe_id_sepomex']\n",
    "vars_x_numerical = [\n",
    "    'first_price', 'diff_first_prediction', \n",
    "    # 'prediction_price_per_square_meter',\n",
    "    # 'surface_total',\n",
    "    'page_on_marketplace'\n",
    "    ]\n",
    "vars_x_binary = []\n",
    "vars_x_geographic = ['latitude', 'longitude']\n",
    "vars_x_time = ['sine_tmonth', 'cosine_tmonth']\n",
    "\n",
    "vars_x_names = vars_x_categorical + vars_x_numerical + vars_x_binary + vars_x_discrete + vars_x_geographic + vars_x_woe + vars_x_time\n",
    "\n",
    "# corroborate there are not duplicates in the vars_x_names\n",
    "print(len(vars_x_names))\n",
    "print(len(set(vars_x_names)))\n",
    "\n",
    "# get y data as sksurv need\n",
    "data_y = np.array(\n",
    "    list(zip(df_model['event'], df_model['time2event'])),\n",
    "    dtype=[('Status', '?'), ('Survival_in_days', '<f8')]\n",
    ")\n",
    "\n",
    "# get x data\n",
    "data_x = (\n",
    "    df_model.copy()\n",
    "    .astype({col: 'category' for col in vars_x_categorical})\n",
    "    .astype({col: np.float64 for col in vars_x_numerical + vars_x_discrete + vars_x_binary + vars_x_geographic + vars_x_woe + vars_x_time})\n",
    "    .astype({col: np.int8 for col in vars_x_binary})\n",
    "    [vars_x_names]\n",
    ")\n",
    "data_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox(X):\n",
    "    # power_transform\n",
    "    power_transform = PowerTransformer(method='yeo-johnson', standardize=True).fit(X)\n",
    "    X_transf = power_transform.transform(X)\n",
    "    return X_transf, power_transform\n",
    "\n",
    "def scale(X):\n",
    "    # power_transform\n",
    "    standard_scaler = StandardScaler().fit(X)\n",
    "    X_transf = standard_scaler.transform(X)\n",
    "    return X_transf, standard_scaler\n",
    "\n",
    "# one hot encoding #\n",
    "data_x_numeric = OneHotEncoder().fit_transform(data_x)\n",
    "colnames_x_numeric = data_x_numeric.columns\n",
    "\n",
    "# get boxcox transformation for each property type\n",
    "boxcox_vars_property = [\n",
    "    'first_price'\n",
    "]\n",
    "# difference between vars_x_numerical and boxcox_vars_property\n",
    "boxcox_vars_all = ['diff_first_prediction', 'page_on_marketplace']\n",
    "# box cox transformation by property type #\n",
    "# subset data\n",
    "idx_house = (data_x_numeric['property_type=house'] >= 1)\n",
    "idx_apartment = (data_x_numeric['property_type=house'] < 1)\n",
    "\n",
    "# get boxcox transformation\n",
    "data_x_numeric.loc[idx_house, boxcox_vars_property], pt_house = boxcox(data_x_numeric.loc[idx_house, boxcox_vars_property])\n",
    "data_x_numeric.loc[idx_apartment, boxcox_vars_property], pt_apartment = boxcox(data_x_numeric.loc[idx_apartment, boxcox_vars_property])\n",
    "data_x_numeric[boxcox_vars_all], pt_all = boxcox(data_x_numeric[boxcox_vars_all])\n",
    "\n",
    "# to numeric\n",
    "data_x_numeric = data_x_numeric.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['diff_first_prediction', 'page_on_marketplace'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_all.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['property_type=house', 'first_price', 'diff_first_prediction',\n",
       "       'page_on_marketplace', 'latitude', 'longitude', 'woe_marketplace',\n",
       "       'woe_seller', 'woe_id_sepomex', 'sine_tmonth', 'cosine_tmonth'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames_x_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_x_numeric, data_y, test_size=0.05, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45027, 11)\n",
      "(2370, 11)\n"
     ]
    }
   ],
   "source": [
    "# print shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add monotonic constraints\n",
    "monotone_constraints = len(colnames_x_numeric) * [0]\n",
    "\n",
    "# add descreasing monotonic constraints for 'first_price'\n",
    "monotone_constraints[colnames_x_numeric.to_list().index('first_price')] = 1\n",
    "# add increasing monotonic constraints for 'diff_first_prediction'\n",
    "monotone_constraints[colnames_x_numeric.to_list().index('diff_first_prediction')] = 1\n",
    "# add increasing monotonic constraints for 'property_type=house'\n",
    "\n",
    "monotone_constraints = tuple(monotone_constraints)\n",
    "monotone_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-aft-nloglik:13.68248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalidation-aft-nloglik:3.27984\n",
      "[100]\tvalidation-aft-nloglik:3.18555\n",
      "[150]\tvalidation-aft-nloglik:3.18202\n",
      "[188]\tvalidation-aft-nloglik:3.18119\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBSEStackedWeibull(weibull_params={},\n",
       "                    xgb_params={&#x27;aft_loss_distribution&#x27;: &#x27;normal&#x27;,\n",
       "                                &#x27;aft_loss_distribution_scale&#x27;: 1,\n",
       "                                &#x27;booster&#x27;: &#x27;dart&#x27;, &#x27;colsample_bynode&#x27;: 0.5,\n",
       "                                &#x27;eval_metric&#x27;: &#x27;aft-nloglik&#x27;,\n",
       "                                &#x27;learning_rate&#x27;: 0.05, &#x27;max_depth&#x27;: 8,\n",
       "                                &#x27;min_child_weight&#x27;: 50,\n",
       "                                &#x27;monotone_constraints&#x27;: (0, 1, 1, 0, 0, 0, 0, 0,\n",
       "                                                         0, 0, 0),\n",
       "                                &#x27;objective&#x27;: &#x27;survival:aft&#x27;, &#x27;subsample&#x27;: 0.5,\n",
       "                                &#x27;tree_method&#x27;: &#x27;hist&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBSEStackedWeibull</label><div class=\"sk-toggleable__content\"><pre>XGBSEStackedWeibull(weibull_params={},\n",
       "                    xgb_params={&#x27;aft_loss_distribution&#x27;: &#x27;normal&#x27;,\n",
       "                                &#x27;aft_loss_distribution_scale&#x27;: 1,\n",
       "                                &#x27;booster&#x27;: &#x27;dart&#x27;, &#x27;colsample_bynode&#x27;: 0.5,\n",
       "                                &#x27;eval_metric&#x27;: &#x27;aft-nloglik&#x27;,\n",
       "                                &#x27;learning_rate&#x27;: 0.05, &#x27;max_depth&#x27;: 8,\n",
       "                                &#x27;min_child_weight&#x27;: 50,\n",
       "                                &#x27;monotone_constraints&#x27;: (0, 1, 1, 0, 0, 0, 0, 0,\n",
       "                                                         0, 0, 0),\n",
       "                                &#x27;objective&#x27;: &#x27;survival:aft&#x27;, &#x27;subsample&#x27;: 0.5,\n",
       "                                &#x27;tree_method&#x27;: &#x27;hist&#x27;})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBSEStackedWeibull(weibull_params={},\n",
       "                    xgb_params={'aft_loss_distribution': 'normal',\n",
       "                                'aft_loss_distribution_scale': 1,\n",
       "                                'booster': 'dart', 'colsample_bynode': 0.5,\n",
       "                                'eval_metric': 'aft-nloglik',\n",
       "                                'learning_rate': 0.05, 'max_depth': 8,\n",
       "                                'min_child_weight': 50,\n",
       "                                'monotone_constraints': (0, 1, 1, 0, 0, 0, 0, 0,\n",
       "                                                         0, 0, 0),\n",
       "                                'objective': 'survival:aft', 'subsample': 0.5,\n",
       "                                'tree_method': 'hist'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit weibull\n",
    "xgboost_params = {\n",
    "            \"objective\": \"survival:aft\",\n",
    "            \"eval_metric\": \"aft-nloglik\",\n",
    "            \"aft_loss_distribution\": \"normal\",\n",
    "            \"aft_loss_distribution_scale\": 1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 5e-2,\n",
    "            \"max_depth\": 8,\n",
    "            \"booster\": \"dart\",\n",
    "            \"subsample\": 0.5,\n",
    "            \"min_child_weight\": 50,\n",
    "            \"colsample_bynode\": 0.5,\n",
    "            'monotone_constraints': monotone_constraints\n",
    "        }\n",
    "xgbse_weibull = XGBSEStackedWeibull(xgb_params=xgboost_params)\n",
    "xgbse_weibull.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=50,\n",
    "    time_bins = range(1, 171, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>cindex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>40.424149</td>\n",
       "      <td>0.719277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>39.239380</td>\n",
       "      <td>0.717420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rmse    cindex\n",
       "property_type                     \n",
       "apartment      40.424149  0.719277\n",
       "house          39.239380  0.717420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_xgbse_mean_time(df):\n",
    "    \"\"\"Get mean time to event for a given time interval.\"\"\"\n",
    "    # get linespace from names of columns\n",
    "    delta = df.columns.astype(int).to_numpy()\n",
    "    # get survival probabilities as the values of the dataframe\n",
    "    surv_probas = df.values\n",
    "\n",
    "    # for each row, compute the area under the curve\n",
    "    mean_time = np.array([simpson(y=y, x=delta) for y in surv_probas])\n",
    "\n",
    "    return(mean_time)\n",
    "\n",
    "def get_metrics(df):\n",
    "    df = df.copy()\n",
    "    cindex = concordance_index_censored(df['event'], df['observed_time'], df['risk_score'])[0]\n",
    "    # rmse & mape for all with event as True\n",
    "    rmse = np.sqrt(np.mean((df[df['event']]['predicted_time'] - df[df['event']]['observed_time'])**2))\n",
    "    return pd.Series({'rmse': rmse, 'cindex': cindex})\n",
    "\n",
    "def get_prediction_df(X, y, colnames, model):\n",
    "    # get rmse, mape and cindex by listing & property type\n",
    "    df_pred = (\n",
    "        pd.DataFrame(X, columns=colnames)\n",
    "        .assign(\n",
    "            observed_time=y['Survival_in_days'],\n",
    "            event=y['Status'],\n",
    "            predicted_time=model.predict(X).pipe(get_xgbse_mean_time),\n",
    "            risk_score=lambda x: - x['predicted_time']\n",
    "        )\n",
    "        .rename(columns={\n",
    "        'property_type=house': 'property_type',\n",
    "        })\n",
    "        .assign(\n",
    "            property_type=lambda x: np.where(x['property_type'] == 1, 'house', 'apartment'),\n",
    "        )  \n",
    "    )\n",
    "\n",
    "    return df_pred\n",
    "\n",
    "# get prediction df\n",
    "df_pred = get_prediction_df(X_test, y_test, colnames_x_numeric, xgbse_weibull)\n",
    "\n",
    "# get metrics\n",
    "table_metrics = (\n",
    "    df_pred\n",
    "    .groupby(['property_type'])\n",
    "    .apply(get_metrics)\n",
    ")\n",
    "table_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://dd360-ds-artifacts/135', creation_time=1700156333492, experiment_id='135', last_update_time=1700156333492, lifecycle_stage='active', name='liquidity-sale-cdmx', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys\n",
    "os.environ[\"AWS_PROFILE\"] = \"default\" # prod\n",
    "\n",
    "# track server\n",
    "TRACKING_SERVER_HOST = \"mlflow.prod.dd360.mx\" # fill in with the public DNS of the EC2 instance\n",
    "\n",
    "# set uri\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:443\")\n",
    "\n",
    "# experiment\n",
    "EXPERIMENT_NAME = \"liquidity-sale-cdmx\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudpickle\n",
    "pt_all_serialized = cloudpickle.dumps(pt_all)\n",
    "pt_house_serialized = cloudpickle.dumps(pt_house)\n",
    "pt_apartment_serialized = cloudpickle.dumps(pt_apartment)\n",
    "xgbse_weibull_serialized = cloudpickle.dumps(xgbse_weibull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/16 11:46:14 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2023/11/16 11:46:18 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "/Users/ravj/opt/anaconda3/envs/dd3surv/lib/python3.9/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/ravj/opt/anaconda3/envs/dd3surv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/11/16 11:46:20 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2023/11/16 11:46:21 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# start run\n",
    "with mlflow.start_run() as run:\n",
    "    # set tags\n",
    "    mlflow.set_tag('model', 'survival')\n",
    "    mlflow.set_tag('model-type', 'xgbse-stacked-weibull')\n",
    "    mlflow.set_tag('model-name', 'liquidity_v1')\n",
    "    mlflow.set_tag('model-version', '1.0.0')\n",
    "    mlflow.set_tag('model-description', 'Modelo de supervivencia para predecir el tiempo de venta de una propiedad')\n",
    "    # log model\n",
    "\n",
    "    # mlflow.log_artifact(xgbse_weibull, 'model')\n",
    "    # log variables\n",
    "    mlflow.log_param('variables', vars_x_names)\n",
    "    mlflow.log_param('categorical_variables', vars_x_categorical)\n",
    "    mlflow.log_param('discrete_variables', vars_x_discrete)\n",
    "    mlflow.log_param('woe_variables', vars_x_woe)\n",
    "    mlflow.log_param('numerical_variables', vars_x_numerical)\n",
    "    mlflow.log_param('binary_variables', vars_x_binary)\n",
    "    mlflow.log_param('geographic_variables', vars_x_geographic)\n",
    "    mlflow.log_param('time_variables', vars_x_time)\n",
    "    # log transformations\n",
    "    mlflow.sklearn.log_model(pt_all_serialized, 'pt_all', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n",
    "    mlflow.sklearn.log_model(pt_house_serialized, 'pt_house', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n",
    "    mlflow.sklearn.log_model(pt_apartment_serialized, 'pt_apartment', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n",
    "    mlflow.sklearn.log_model(xgbse_weibull_serialized, 'xgbse_weibull', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n",
    "    # log all the table_metrics\n",
    "    for index, row in table_metrics.iterrows():\n",
    "        mlflow.log_metric(f\"rmse_{index[0]}_{index[1]}\", row['rmse'])\n",
    "        mlflow.log_metric(f\"cindex_{index[0]}_{index[1]}\", row['cindex'])\n",
    "    \n",
    "# end run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get log id\n",
    "# log_id = \"39d3eadaedf5499d9051fdfa94bd6994\"\n",
    "\n",
    "# # load models #\n",
    "# # load power transform\n",
    "# power_transform_load = cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/power_transform\"))\n",
    "# # load standard scaler\n",
    "# standard_scaler_load = cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/standard_scaler\"))\n",
    "# # # load xgbse weibull\n",
    "# xgbse_weibull_load =  cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/xgbse_weibull\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look transformed data is the same as original\n",
    "# data_aux = data_x[vars_x_discrete + vars_x_geographic].copy()\n",
    "# (pd.DataFrame(standard_scaler_load.inverse_transform(data_x_numeric_aux_scale), columns=location_cols_scale) - pd.DataFrame(standard_scaler_load.inverse_transform(data_x_numeric_aux_scale), columns=location_cols_scale)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbse_weibull_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one value\n",
    "data_x.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_models(log_id):\n",
    "#     \"\"\"\n",
    "#     Load models from mlflow\n",
    "#     \"\"\"\n",
    "#     # get the model\n",
    "\n",
    "#     # load power transform\n",
    "#     power_transform_load = cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/power_transform\"))\n",
    "#     # load standard scaler\n",
    "#     standard_scaler_load = cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/standard_scaler\"))\n",
    "#     # # load xgbse weibull\n",
    "#     xgbse_weibull_load =  cloudpickle.loads(mlflow.sklearn.load_model(f\"runs:/{log_id}/xgbse_weibull\"))\n",
    "\n",
    "#     # save them into a dictionary\n",
    "#     models = {\n",
    "#         \"power_transform\": power_transform_load,\n",
    "#         \"standard_scaler\": standard_scaler_load,\n",
    "#         \"xgbse_weibull\": xgbse_weibull_load\n",
    "#     }\n",
    "#     return models\n",
    "\n",
    "# LOG_ID = \"39d3eadaedf5499d9051fdfa94bd6994\"\n",
    "# models = load_models(LOG_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to cloudpickle\n",
    "with open('models/pt_all.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(pt_all, f)\n",
    "\n",
    "with open('models/pt_house.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(pt_house, f)\n",
    "\n",
    "with open('models/pt_apartment.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(pt_apartment, f)\n",
    "\n",
    "with open('models/xgbse_weibull.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(xgbse_weibull, f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd3surv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
